{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyushSHK/TwoTowerThesisProject/blob/main/TwoTowerModelGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChjuaQjm_iBf"
      },
      "source": [
        "Copyright 2020 The TensorFlow Authors.\n",
        "\n",
        "As per the Apache license requirements, this model is a derivative work of the official Tensorflow tutorial and such I have left the original copyright mark in accordance with the Apache license rules. All additions the model are by me. I have also left a copy of the apache notice as per the license."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWqCArLO_kez"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgFBaQZEbw3O"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # May need to run this code in colab to restart the kernel if the packages below are not showing up. Try the ones below first\n",
        "!pip install -q tensorflow-recommenders\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "h_6mm2afob9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbwMjnLP5nZ_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "import datetime"
      ],
      "metadata": {
        "id": "TzT4QnSmv94v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgKIjpQLAiax"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc2REbOO52Fl"
      },
      "outputs": [],
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\") # load tensorflow dataset ratings and movies\n",
        "ratedf=ratings\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"timestamp\": x[\"timestamp\"],\n",
        "    \"occ_label\": x[\"user_occupation_label\"],\n",
        "    \"age\":x[\"bucketized_user_age\"],\n",
        "    \"gen\":(tf.where(x[\"user_gender\"],1.0,0.0)),\n",
        "    \"occtext\":tf.strings.regex_replace(x[\"user_occupation_text\"],\"/\",\" \"),\n",
        "    \"genres\":x['movie_genres']\n",
        "}) # create dictionary of values from tensorflow dataset change the format of some features\n",
        "\n",
        "\n",
        "\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import unique\n",
        "import pandas as pd\n",
        "df2=tfds.as_dataframe(ratings) # turn into dataframe\n",
        "df=df2\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "E1SVj0T0KiYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df2=np.array2string(df[\"genres\"].values)\n",
        "\n",
        "print(type(df[\"genres\"][0]))\n",
        "df['genres'] = df['genres'].apply(lambda x: np.array2string(x)) \n",
        "df[\"genres\"] = df[\"genres\"].str.replace('[','')\n",
        "df[\"genres\"] = df[\"genres\"].str.replace(']','')\n",
        "df[\"genres\"] = df[\"genres\"].str.replace('b','')\n",
        "df[\"genres\"] = df[\"genres\"].str.replace(\"'\",'')\n",
        "df[\"genres\"] = df[\"genres\"].str.replace(\",\",\" \")\n",
        "df.head() #convert genres to strings and strip uneccesary characters"
      ],
      "metadata": {
        "id": "NozPO-yHKtaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(#create custom dataset from amended dataframe\n",
        "    ({\n",
        "      \"age\":df['age'].values, \n",
        "      \"gen\":df['gen'].values,\n",
        "      \"genres\":df['genres'].values,\n",
        "      \"movie_title\":df['movie_title'].values,\n",
        "      \"occ_label\":df['occ_label'].values,\n",
        "      \"occtext\":df['occtext'].values,\n",
        "      \"timestamp\":df['timestamp'].values,\n",
        "      \"user_id\":df['user_id'].values}))"
      ],
      "metadata": {
        "id": "iX8YdOahNVAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.element_spec"
      ],
      "metadata": {
        "id": "7QskFtv3Nfgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas = dataset.map(lambda x: {#Custome dataset back to a dict\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"timestamp\": x[\"timestamp\"],\n",
        "    \"occ_label\": x[\"occ_label\"],\n",
        "    \"age\":x[\"age\"],\n",
        "    \"gen\":x[\"gen\"],\n",
        "    \"occtext\":tf.strings.regex_replace(x[\"occtext\"],\"/\",\" \"),\n",
        "    \"genres\":x[\"genres\"],\n",
        "})"
      ],
      "metadata": {
        "id": "nMpc7VUtGLHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZ2q5RXYNI6"
      },
      "source": [
        "Prepare buckets and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamps = np.concatenate(list(datas.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
        "\n",
        "max_timestamp = timestamps.max()\n",
        "min_timestamp = timestamps.min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000,\n",
        ")\n",
        "ages = np.concatenate(list(datas.map(lambda x: x[\"age\"]).batch(100)))\n",
        "\n",
        "max_age = ages.max()\n",
        "min_age = ages.min()\n",
        "age_buckets = np.linspace(\n",
        "    min_age, max_age, num=20,\n",
        ")\n",
        "genress=datas.map(lambda x: x[\"genres\"]).batch(100)\n",
        "occtexts=datas.map(lambda x: x[\"occtext\"]).batch(100)\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(datas.batch(1_000).map(\n",
        "    lambda x: x[\"user_id\"]))))\n",
        "unique_occ_ids = np.unique(np.concatenate(list(datas.batch(1_000).map(\n",
        "    lambda x: x[\"occ_label\"]))))\n",
        "unique_gen = np.unique(np.concatenate(list(datas.batch(1_000).map(\n",
        "    lambda x: x[\"gen\"]))))"
      ],
      "metadata": {
        "id": "SqfYIVV0J36n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJcCVMUQou3"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ItzYwMW42cb"
      },
      "outputs": [],
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "#all the encoding and preprocessing for all the query features\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=unique_user_ids, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
        "    ])\n",
        "    self.gen_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.IntegerLookup(\n",
        "            vocabulary=unique_gen, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(unique_gen) + 1, 1),\n",
        "    ])\n",
        "\n",
        "    self.occ_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.IntegerLookup(\n",
        "            vocabulary=unique_occ_ids, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(unique_occ_ids) + 1, 32),\n",
        "    ])\n",
        "    self.timestamp_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
        "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
        "    ])\n",
        "    self.normalized_timestamp = tf.keras.layers.Normalization(\n",
        "        axis=None\n",
        "    )\n",
        "    self.age_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.Discretization(age_buckets.tolist()),\n",
        "        tf.keras.layers.Embedding(len(age_buckets) + 1, 32),\n",
        "    ])\n",
        "    self.normalized_age = tf.keras.layers.Normalization(\n",
        "        axis=None\n",
        "    )\n",
        "    self.occ_vectorizer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=100)\n",
        "\n",
        "    self.occ_text_embedding = tf.keras.Sequential([\n",
        "      self.occ_vectorizer,\n",
        "      tf.keras.layers.Embedding(100, 32, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.genre_vectorizer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=100)\n",
        "\n",
        "    self.genre_embedding = tf.keras.Sequential([\n",
        "      self.genre_vectorizer,\n",
        "      tf.keras.layers.Embedding(22, 32, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.genre_vectorizer.adapt(genress)\n",
        "    self.occ_vectorizer.adapt(occtexts)\n",
        "    self.normalized_timestamp.adapt(timestamps)\n",
        "    self.normalized_age.adapt(ages)\n",
        "  def getResult(self,inputs): # returns a copy of the result for intermediate neuron activation\n",
        "        return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        self.occ_embedding(inputs[\"occ_label\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
        "        self.age_embedding(inputs[\"age\"]),\n",
        "        tf.reshape(self.normalized_age(inputs[\"age\"]), (-1, 1)),\n",
        "        self.gen_embedding(inputs[\"gen\"]),\n",
        "        self.occ_text_embedding(inputs[\"occtext\"]),\n",
        "        self.genre_embedding(inputs[\"genres\"]),\n",
        "    ], axis=1)\n",
        "  def call(self, inputs):\n",
        "    # Take the input dictionary, pass it through each input layer,\n",
        "    # and concatenate the result.\n",
        "    return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        self.occ_embedding(inputs[\"occ_label\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
        "        self.age_embedding(inputs[\"age\"]),\n",
        "        tf.reshape(self.normalized_age(inputs[\"age\"]), (-1, 1)),\n",
        "        self.gen_embedding(inputs[\"gen\"]),\n",
        "        self.occ_text_embedding(inputs[\"occtext\"]),\n",
        "        self.genre_embedding(inputs[\"genres\"]),\n",
        "    ], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qfPi4I-Z0ph"
      },
      "outputs": [],
      "source": [
        "class QueryModel(tf.keras.Model):\n",
        " \n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "  \n",
        "    self.embedding_model = UserModel()\n",
        "\n",
        "\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "    \n",
        "    self.dense_layers.add(tf.keras.layers.Reshape((2,16)))\n",
        "    self.dense_layers.add(tf.keras.layers.Conv1D(\n",
        "        filters=32, kernel_size=1, strides=1, padding=\"same\", activation=\"relu\"))\n",
        "    self.dense_layers.add(tf.keras.layers.GlobalMaxPool1D(\n",
        "    data_format='channels_last', keepdims=False\n",
        ")) \n",
        "  \n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "  \n",
        "  def getUser(self): #needed to access sub model\n",
        "    return self.embedding_model  \n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQZHX8bEHPOk"
      },
      "outputs": [],
      "source": [
        "class MovieModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    max_tokens = 10_000\n",
        "\n",
        "    self.title_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "          vocabulary=unique_movie_titles,mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=max_tokens)\n",
        "\n",
        "    self.title_text_embedding = tf.keras.Sequential([\n",
        "      self.title_vectorizer,\n",
        "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer.adapt(movies)\n",
        "  def getResult(self,titles):\n",
        "    return tf.concat([\n",
        "        self.title_embedding(titles),\n",
        "        self.title_text_embedding(titles),\n",
        "    ], axis=1)\n",
        "  def call(self, titles):\n",
        "    return tf.concat([\n",
        "        self.title_embedding(titles),\n",
        "        self.title_text_embedding(titles),\n",
        "    ], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1gTXkvQqHGA"
      },
      "outputs": [],
      "source": [
        "class CandidateModel(tf.keras.Model):\n",
        "\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_model = MovieModel()\n",
        "\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "  \n",
        "    self.dense_layers.add(tf.keras.layers.Reshape((2,16)))\n",
        "    self.dense_layers.add(tf.keras.layers.Conv1D(\n",
        "        filters=32, kernel_size=1, strides=1, padding=\"same\", activation=\"relu\"))\n",
        "    self.dense_layers.add(tf.keras.layers.GlobalMaxPool1D(\n",
        "    data_format='channels_last', keepdims=False\n",
        ")) # Mirrors other tower\n",
        "    \n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "  def getMovie(self):\n",
        "    return self.embedding_model\n",
        "      \n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26_hNJPKIh4-"
      },
      "outputs": [],
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    super().__init__()\n",
        "    self.query_model = QueryModel(layer_sizes)\n",
        "    self.candidate_model = CandidateModel(layer_sizes)\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.candidate_model),\n",
        "        ),\n",
        "    )\n",
        "  def getMovie(self):\n",
        "    return self.candidate_model\n",
        "  def getUser(self):\n",
        "    return self.query_model\n",
        "  def compute_loss(self, features, training=False):\n",
        "\n",
        "    query_embeddings = self.query_model({\n",
        "        \"user_id\": features[\"user_id\"],\n",
        "        \"timestamp\": features[\"timestamp\"],\n",
        "        \"occ_label\": features[\"occ_label\"],\n",
        "        \"age\":features[\"age\"],\n",
        "        \"gen\":features[\"gen\"],\n",
        "        \"occtext\":features[\"occtext\"],\n",
        "        \"genres\":features[\"genres\"],\n",
        "    })\n",
        "    movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
        "\n",
        "    return self.task(\n",
        "        query_embeddings, movie_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMFUZ4dyTdYd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train+validation data\n",
        "shuffled = datas.shuffle(100000, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80000)\n",
        "test = shuffled.skip(80000).take(20000)\n",
        "\n",
        "cached_train = train.shuffle(100000).batch(2048)\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one_layer_history.history.keys() "
      ],
      "metadata": {
        "id": "iI_7RDMLsyNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkoLkiQdK4Um"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50 # 30 min run time approximately, only works in non gpu runtime\n",
        "\n",
        "model = MovielensModel([64,32,16])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #Tensorboard logs\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=True)\n",
        "\n",
        "one_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0,\n",
        "    callbacks=[tensorboard_callback])\n",
        "model.fit(cached_train,epochs=num_epochs)\n",
        "accuracy = one_layer_history.history[\"factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''writer = tf.summary.create_file_writer('./graphs')\n",
        "tf.summary.trace_on(graph=True, profiler=True)\n",
        "with writer.as_default():\n",
        "  tf.summary.trace_export(\n",
        "      name=\"trace\",\n",
        "      step=0,\n",
        "      profiler_outdir='./graphs')\n",
        "tf.summary.trace_on(graph=True, profiler=True)'''\n",
        "\n",
        "%tensorboard --logdir logs/fit\n"
      ],
      "metadata": {
        "id": "C2UuAfaEywLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brute force layer to retrieve candidates\n",
        "brute_force = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
        "brute_force.index_from_dataset(\n",
        "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.candidate_model)))\n",
        ")\n",
        "#brute_force.index(movies.batch(128).map(model.candidate_model), movies)\n",
        "_, titles = brute_force({\n",
        "    \"user_id\": np.array([\"996\"]),\n",
        "    \"timestamp\": np.array([879024327]),\n",
        "    \"occ_label\": np.array([17]),\n",
        "    \"age\": np.array([27.0]),\n",
        "    \"gen\": np.array([0.0]),\n",
        "    \"occtext\": np.array([\"student\"]),\n",
        "    \"genres\":np.array([\"4 10\"])\n",
        "    },\n",
        "    k=20\n",
        ")\n",
        "\n",
        "print(titles)"
      ],
      "metadata": {
        "id": "iO4aAIE4weLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Neuron extraction\n",
        "from keras import backend as K\n",
        "Layerout=[]\n",
        "movie=model.getMovie()\n",
        "#movie.get_layer(name=\"sequential_11\").summary()\n",
        "user=model.getUser()\n",
        "user.summary()\n",
        "#user.get_layer(name=\"sequential_8\").summary()\n",
        "Seqname=\"sequential_7\" #Normally defaults to this but if you run the model twice it changes!\n",
        "user.get_layer(name=Seqname).summary()\n",
        "\n",
        "input={\n",
        "    \"user_id\": np.array([\"996\"]),\n",
        "    \"timestamp\": np.array([879024327]),\n",
        "    \"occ_label\": np.array([4]),\n",
        "    \"age\": np.array([55.0]),\n",
        "    \"gen\": np.array([0.0]),\n",
        "    \"occtext\": np.array([\"doctor\"]),\n",
        "    \"genres\":np.array([\"4 10\"])\n",
        "    \n",
        "    } # need to manually change inputs to whatver combination to get all query side activations\n",
        "res=user.getUser().getResult(input)\n",
        "\n",
        "\n",
        "#from keras import backend as K\n",
        "\n",
        "get_1st = K.function(\n",
        "  [user.get_layer(name=Seqname).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [user.get_layer(name=Seqname).layers[0].output]) r\n",
        "#Need an individual function for each layer\n",
        "\n",
        "get_2nd = K.function(\n",
        "  [user.get_layer(name=Seqname).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [user.get_layer(name=Seqname).layers[1].output]) \n",
        "\n",
        "get_3rd = K.function(\n",
        "  [user.get_layer(name=Seqname).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [user.get_layer(name=Seqname).layers[2].output]) \n",
        "\n",
        "\n",
        "get_4th = K.function(\n",
        "  [user.get_layer(name=Seqname).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [user.get_layer(name=Seqname).layers[3].output]) \n",
        "\n",
        "get_5th = K.function(\n",
        "  [user.get_layer(name=Seqname).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [user.get_layer(name=Seqname).layers[4].output]) \n",
        "\n",
        "get_6th = K.function(\n",
        "  [user.get_layer(name=Seqname).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [user.get_layer(name=Seqname).layers[5].output])\n",
        "fin=[]\n",
        "# here X is param 1 (input) and the function returns output from layers[3]\n",
        "output1 = get_1st(res)[0]\n",
        "output2= get_2nd(res)[0]\n",
        "output3= get_6th(res)[0]\n",
        "outputConv=get_5th(res)[0]\n",
        "\n",
        "fin=np.concatenate([output1[0], output2[0],outputConv[0]])\n",
        "print(len(fin))"
      ],
      "metadata": {
        "id": "n-qRDDdQoq0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputMov={\n",
        "    \"movie_title\": np.array([\"b\\'Braindead (1992)'\\\"\"])}\n",
        "#inputMov=\"b'Second Jungle Book: Mowgli & Baloo, The (1997)'\"\"    \n",
        "# need to manually change inputs to whatver combination to get all movie side activations\n",
        "resM=movie.getMovie().getResult(inputMov)\n",
        "SeqM=\"sequential_10\" #Normally defaults to this but if you run the model twice it changes!\n",
        "movie.summary()\n",
        "movie.get_layer(name=SeqM).summary()\n",
        "\n",
        "get_1stM = K.function(\n",
        "  [movie.get_layer(name=SeqM).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [movie.get_layer(name=SeqM).layers[0].output]) \n",
        "\n",
        "\n",
        "get_2ndM = K.function(\n",
        "  [movie.get_layer(name=SeqM).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [movie.get_layer(name=SeqM).layers[1].output]) \n",
        "\n",
        "get_3rdM = K.function(\n",
        "  [movie.get_layer(name=SeqM).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [movie.get_layer(name=SeqM).layers[2].output]) \n",
        "\n",
        "get_4thM = K.function(\n",
        "  [movie.get_layer(name=SeqM).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [movie.get_layer(name=SeqM).layers[3].output]) \n",
        "\n",
        "get_5thM = K.function(\n",
        "  [movie.get_layer(name=SeqM).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [movie.get_layer(name=SeqM).layers[4].output]) \n",
        "\n",
        "get_6thM = K.function(\n",
        "  [movie.get_layer(name=SeqM).layers[0].input], # param 1 will be treated as layer[0].output\n",
        "  [movie.get_layer(name=SeqM).layers[5].output]) \n",
        "\n",
        "finM=[]\n",
        "\n",
        "output1M = get_1stM(resM)[0]\n",
        "output2M= get_2ndM(resM)[0]\n",
        "output3M= get_6thM(resM)[0]\n",
        "outputConvM=get_5thM(resM)[0]\n",
        "finM=np.concatenate([output1M[0], output2M[0],outputConvM[0]])\n",
        "print(finM)"
      ],
      "metadata": {
        "id": "J91wzbI4qjrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp=np.dot(np.squeeze(output3),np.squeeze(output3M))\n",
        "dp=np.dot(np.squeeze(output3),np.squeeze(output3M))\n",
        "print(dp)\n",
        "#computes dot product and a sigmoid values\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "#print(softmax(dp))\n",
        "print(sigmoid(dp))"
      ],
      "metadata": {
        "id": "5xcJ2iHpoARD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row1=np.concatenate([fin,finM,[dp],[sigmoid(dp)]]) #creates neuron row, used if first row in a new set of rows"
      ],
      "metadata": {
        "id": "qiP8fxmJBA5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#row1=np.concatenate([fin,finM,[dp],[sigmoid(dp)]])\n",
        "row2=np.concatenate([fin,finM,[dp],[sigmoid(dp)]]) #use if second+ row\n",
        "print(row1)\n",
        "row1=np.vstack([row1,row2])"
      ],
      "metadata": {
        "id": "-aLYpjIciz3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import scipy"
      ],
      "metadata": {
        "id": "CcQXXWPKlWPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#row1=np.vstack([row1,fin])\n",
        "coeff=np.corrcoef(row1,rowvar=False) #correlation coefficent matrix creation\n",
        "coeffFin=np.nan_to_num(coeff)\n",
        "print(np.shape(coeffFin))\n",
        "coeffFin=np.round(coeffFin, 3)\n",
        "q3=np.percentile(coeffFin,97)\n",
        "#q3=0.5\n",
        "print(q3)\n",
        "coeffFin[coeffFin < 0] = 0\n",
        "coeffFin[coeffFin < q3] = 0\n",
        "row1=np.round(row1, 3) #3 s f\n",
        "print(coeffFin)\n",
        "np.savetxt(\"foo.csv\", coeffFin, delimiter=\",\",fmt='%f') #in case you want to see values\n",
        "np.savetxt(\"trial1.csv\", row1, delimiter=\",\",fmt='%f')"
      ],
      "metadata": {
        "id": "fRcNp_cFR1rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PQerVwPeyTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-igraph==0.8.3 \n",
        "!apt install libcairo2-dev pkg-config python3-dev\n",
        "!pip install python-igraph leidenalg cairocffi"
      ],
      "metadata": {
        "id": "cXATbQM_igjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import igraph as ig\n",
        "import cairocffi\n",
        "\n",
        "with open('trial1.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    vertex = list(reader)\n",
        "\n",
        "print(vertex)\n",
        "\n",
        "with open('foo.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    edges = list(reader)\n",
        "\n",
        "\n",
        "G=ig.Graph.Weighted_Adjacency(coeffFin ,mode='undirected',  attr='weight', loops=False) # create graph\n",
        "G.vs[0:63][\"layer\"]=\"query-Dense 1\"\n",
        "G.vs[64:95][\"layer\"]=\"query-Dense 2\"\n",
        "G.vs[96:127][\"layer\"]=\"query-Pool\"\n",
        "G.vs[128:191][\"layer\"]=\"Movie-Dense 1\"\n",
        "G.vs[192:223][\"layer\"]=\"Movie-Dense 2\"\n",
        "G.vs[204:255][\"layer\"]=\"Movie-Pool\"\n",
        "G.vs[256][\"layer\"]=\"DotProduct\"\n",
        "G.vs[257][\"layer\"]=\"Sigmoid Output\"\n",
        "G.vs[0:63][\"color\"]=\"blue\"\n",
        "G.vs[64:95][\"color\"]=\"cyan\"\n",
        "G.vs[96:127][\"color\"]=\"magenta\"\n",
        "G.vs[128:191][\"color\"]=\"red\"\n",
        "G.vs[192:223][\"color\"]=\"orange\"\n",
        "G.vs[204:255][\"color\"]=\"yellow\"\n",
        "G.vs[256][\"color\"]=\"purple\"\n",
        "G.vs[257][\"color\"]=\"green\"\n",
        "G.vs[\"label\"] = range(G.vcount())\n",
        "ig.plot(G,vertex_color = G.vs['color'],vertex_label=G.vs[\"label\"])"
      ],
      "metadata": {
        "id": "ROCcAQF4bllA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename=\"ComHorrPos\" # set file names"
      ],
      "metadata": {
        "id": "OvYtzgc3lo5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vseq = G.vs #delete unconnected nodes\n",
        "to_delete_ids = [v.index for v in G.vs if v.degree() == 0]\n",
        "G.delete_vertices(to_delete_ids)\n",
        "ig.plot(G,filename+\"Full.png\",vertex_color = G.vs['color'],vertex_label=G.vs[\"label\"],vertex_label_size=10)\n"
      ],
      "metadata": {
        "id": "tmpdXGtEmLtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comms = G.community_multilevel(weights=\"weight\")\n",
        "print(comms.modularity)\n",
        "print(comms)\n",
        "G.vs[\"group\"] = comms.membership\n",
        "membership=[]\n",
        "for i,mem in enumerate(comms.membership):\n",
        "  cluster=[]\n",
        "  for j,item in enumerate(G.vs[\"group\"]):\n",
        "    if G.vs[\"group\"][j]==i:\n",
        "      cluster.append(G.vs[\"label\"][j])\n",
        "  if len(cluster)==0:\n",
        "    break\n",
        "  membership.append(cluster)\n",
        "Loudf = pd.DataFrame.from_records(membership)  #Get communities \n",
        "ig.plot(comms,filename+\"Lou.png\", mark_groups = True,vertex_color = G.vs['color'],vertex_label=G.vs[\"label\"],vertex_label_size=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "8yeAHeoa5exd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comms = G.community_leiden(objective_function='modularity',weights=\"weight\")\n",
        "print(comms.modularity)\n",
        "membership=[]\n",
        "for i,mem in enumerate(comms.membership):\n",
        "  cluster=[]\n",
        "  for j,item in enumerate(G.vs[\"group\"]):\n",
        "    if G.vs[\"group\"][j]==i:\n",
        "      cluster.append(G.vs[\"label\"][j])\n",
        "  if len(cluster)==0:\n",
        "    break\n",
        "  membership.append(cluster)\n",
        "Leidf = pd.DataFrame.from_records(membership)#Get communities \n",
        "ig.plot(comms,filename+\"Lei.png\" ,mark_groups = True,vertex_color = G.vs['color'],vertex_label=G.vs[\"label\"],vertex_label_size=10)"
      ],
      "metadata": {
        "id": "kHtXefWO-NBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Leidf.head()"
      ],
      "metadata": {
        "id": "cHMXDwoqygy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loudf.fillna('') #clean and save membership to csv\n",
        "\n",
        "Loudf.to_csv(filename+\"Lou.csv\",index=False)\n"
      ],
      "metadata": {
        "id": "GUGt_2Lb6z3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Leidf.fillna('') #clean and save membership to csv\n",
        "Leidf.to_csv(filename+\"Lei.csv\",index=False)"
      ],
      "metadata": {
        "id": "31PGpvMR7Cjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pr=G.pagerank(weights='weight')\n",
        "\n",
        "vc=[G.vertex_connectivity()]\n",
        "dens=[G.density()]\n",
        "bet=G.betweenness(weights='weight')\n",
        "vlab=G.vs[\"label\"]\n",
        "close=G.closeness(weights='weight')\n",
        "print(dens)\n",
        "print(vc)\n",
        "\n",
        "Indexs=[]\n",
        "for v in G.vs:\n",
        "  Indexs.append(v.index)\n",
        "\n",
        "Gdf = pd.DataFrame(list(zip(Indexs,vlab, pr,bet,close)),\n",
        "               columns =['Node ID','Original Node ID', 'Page Rank Centrality','Betweeness','Closeness Centrality'])\n",
        "\n",
        "Gdf.to_csv(filename+\"Measures.csv\",index=False) # get all centrality information and save to df\n",
        "ig.plot(G, vertex_color = G.vs['color'],vertex_label=pr,vertex_label_size=10)"
      ],
      "metadata": {
        "id": "XJliVV99-nRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gdf.head()"
      ],
      "metadata": {
        "id": "J7kxPGIlJQE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ig.summary(G)\n",
        "#G.write_edgelist(\"edges.txt\")\n",
        "G.write_pajek(filename+\"Paj.txt\")\t#get paj file for compnet use\n",
        "  #all similarity calculations are done in compnet"
      ],
      "metadata": {
        "id": "ofLy3o740HQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}